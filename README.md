# Caption Match üñºÔ∏è

A simple application powered by [Superlinked](https://github.com/superlinked/superlinked) to search for images in your library based on caption and other parameters.

### Disclaimer üö®

This project is an experimental exploration of Superlinked's capabilities.
The code is not optimized and has been tested with fewer than 100 personal photos.
Please, treat it as PoC.

### Ideation üí°

Searching for a specific image in a disorganized photo library can be challenging.
Often, users want to locate images using brief descriptions.
To address this, I've developed a tiny application that offers a straightforward interface for conducting such searches across a photo library stored on a hard drive.

### Approach üõ†Ô∏è

**Storing and retrieval**
- [OpenAI CLIP](https://github.com/openai/CLIP) for embedding images and captions. CLIP is a powerful model that can understand and generate embeddings for both images and text in a unified manner. This allows us to create a shared representation for images and their associated captions, enabling efficient and semantically meaningful retrieval.

- [Superlinked](https://github.com/superlinked/superlinked) for quick querying images by caption and other parameters. Superlinked is a versatile framework that provides out-of-the-box vector spaces, in-memory vector storage, and querying. This makes it an ideal choice for storing and retrieving our image-caption pairs.

**Reranking**

- [OpenAI API](https://platform.openai.com/) for generating rich descriptions for images. While CLIP is fast and open-source, it alone may not suffice for building a high-performing retrieval system. Therefore, we employ the OpenAI API to generate more detailed and nuanced descriptions for candidate images, which aids in the reranking process.

- [SentenceTransformers](https://sbert.net) for a final reranking given original caption and automatically generated description for candidates by OpenAI GPT-4. SentenceTransformers allows us to generate meaningful sentence embeddings, which we use to compare the original caption with the descriptions generated by GPT-4. This comparison helps us to rerank the candidate images and select the most relevant ones.

**Other**
- [Streamlit](https://streamlit.io/) for creating a simple UI. Streamlit is a fast, user-friendly tool that allows us to create a UI for our project. This makes it easier to tweak parameters and send queries, providing a more interactive and accessible way to use our image retrieval system.

![Example Image](./misc/system-overview.png)

### Setup and Run üöÄ

1. Put your photos (`.png` and `.jpg`) in [`/data`](./data/) and update environmenta variable `DYNACONF_PHOTO_FOLDER` in [docker-compose.yaml](./docker-compose.yaml) if needed. Be sure that your photos have exif info! Also, you can extract exemplary photos.zip.
1. Build docker image and run docker compose:

```shell
docker build -t caption-match-app .
docker compose up
# go to localhost:8501
```

### Example

**Retrieval**

Let's examine the images retrieved using the caption "Photo of a dog laying on its back on a sofa".

![Example Image](./misc/retrieval.png)

- Image 1 is the most relevant.
- Images 2 and 3 are somewhat relevant, but they depict a cat instead of a dog.
- Image 4 shows a cat (not a dog) laying in a normal position (not on its back) on a sofa.
- Images 5 and 6 are photos of dogs, but they are not laying on their backs, and there is no sofa present.

![Example Image](./misc/reranking.png)

Upon clicking the "Rerank" button, a call is made to OpenAI for the description, followed by the reranking process using CrossEncoder. Here are the reranked results:

- Image 1 remains the most relevant, which is excellent.
- Image 2 received the description "A *dog* is stretched out ...", indicating a misclassification of the animal. If this cat were a dog, it would be a great example of effective reranking.
- The remaining images retain their original positions.

While this specific example may not seem impressive, it demonstrates an important aspect: *the reranking score*. The reranking score can be used to filter out completely irrelevant images, such as the last three in our case. These images received a reranking score of less than 0.1. However, the retrieval score for these irrelevant cases is just slightly lower than for the relevant ones.

### Notes üìù

- Retrieval often yields irrelevant photos, which can be rectified through reranking. My reranking method generates descriptions and uses a CrossEncoder for scoring. However, the non-deterministic nature of generated descriptions [source](https://platform.openai.com/docs/guides/text-generation/reproducible-outputs) poses a challenge for production. Potential solutions include caching descriptions or deploying a dedicated model for reproducibility control.

- Initially, I queried photos using additional parameters like brightness and recency. However, these parameters were set aside due to complications:

  - While easy to add, parameters like brightness, dominant color, and saturation interfere with the caption during querying. The caption's weight becomes less impactful compared to brightness and recency. This could be due to the larger vector space for image embeddings and the potential overlap of these parameters with the caption content.

  - Reranking becomes complex with multiple parameters.

- Currently, data is loaded and processed at each startup. Efficiency could be improved by storing embeddings in a persistent vector database for quick access upon restarts.

- As this is a proof-of-concept, tasks like model selection, proper ETL, and others are pending. These tasks are on the ToDo list.

### ToDo üìå

- [x] Dockerfile
- [x] poetry
- [x] Refactoring
- [ ] Try different "CLIP" model like [SigLIP](https://huggingface.co/google/siglip-so400m-patch14-384), evaluate performance and pick best model
- [ ] Same with the CrossEncoder
- [ ] persistent vector DB
- [ ] docstrings
- [ ] logging
